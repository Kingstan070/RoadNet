{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd7ae1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:40.914535Z",
     "iopub.status.busy": "2025-01-27T17:10:40.914147Z",
     "iopub.status.idle": "2025-01-27T17:10:54.788456Z",
     "shell.execute_reply": "2025-01-27T17:10:54.787470Z"
    },
    "papermill": {
     "duration": 13.880198,
     "end_time": "2025-01-27T17:10:54.789891",
     "exception": false,
     "start_time": "2025-01-27T17:10:40.909693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa165da7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:54.797145Z",
     "iopub.status.busy": "2025-01-27T17:10:54.796683Z",
     "iopub.status.idle": "2025-01-27T17:10:54.800753Z",
     "shell.execute_reply": "2025-01-27T17:10:54.800145Z"
    },
    "papermill": {
     "duration": 0.008787,
     "end_time": "2025-01-27T17:10:54.801994",
     "exception": false,
     "start_time": "2025-01-27T17:10:54.793207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 16\n",
    "COLOR_TO_SEGMENT = [255, 0, 0]\n",
    "LOG_DIR = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/\"\n",
    "EPOCHS = 5\n",
    "\n",
    "IMAGE_DIR = '/kaggle/input/indian-driving-dataset-segmentation-all-level1id/idd/train/images'\n",
    "MASK_DIR = '/kaggle/input/indian-driving-dataset-segmentation-all-level1id/idd/train/rgb_labels'\n",
    "TEST_IMAGE_DIR = '/kaggle/input/indian-driving-dataset-segmentation-all-level1id/idd/val/images'\n",
    "TEST_MASK_DIR = '/kaggle/input/indian-driving-dataset-segmentation-all-level1id/idd/val/rgb_labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b16832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:54.808362Z",
     "iopub.status.busy": "2025-01-27T17:10:54.808106Z",
     "iopub.status.idle": "2025-01-27T17:10:54.814178Z",
     "shell.execute_reply": "2025-01-27T17:10:54.813437Z"
    },
    "papermill": {
     "duration": 0.010597,
     "end_time": "2025-01-27T17:10:54.815411",
     "exception": false,
     "start_time": "2025-01-27T17:10:54.804814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_mask(mask_path):\n",
    "    mask = load_img(mask_path, target_size=IMAGE_SIZE)\n",
    "    mask = img_to_array(mask)\n",
    "    binary_mask = np.all(mask == COLOR_TO_SEGMENT, axis=-1).astype(np.float32)\n",
    "    return binary_mask[..., np.newaxis]\n",
    "\n",
    "\n",
    "def data_generator(image_dir, mask_dir, batch_size):\n",
    "    image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)])\n",
    "    mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)])\n",
    "\n",
    "    while True:  # Infinite loop over the data\n",
    "        images = []\n",
    "        masks = []\n",
    "        for i in range(len(image_paths)):\n",
    "            img = preprocess_image(image_paths[i])\n",
    "            mask = preprocess_mask(mask_paths[i])\n",
    "\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "\n",
    "            if len(images) == batch_size:\n",
    "                yield np.array(images), np.array(masks)  # Return a batch\n",
    "                images = []  # Reset for the next batch\n",
    "                masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0300460c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:54.821777Z",
     "iopub.status.busy": "2025-01-27T17:10:54.821572Z",
     "iopub.status.idle": "2025-01-27T17:10:54.826993Z",
     "shell.execute_reply": "2025-01-27T17:10:54.826387Z"
    },
    "papermill": {
     "duration": 0.009911,
     "end_time": "2025-01-27T17:10:54.828197",
     "exception": false,
     "start_time": "2025-01-27T17:10:54.818286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_deeplabv3plus_model():\n",
    "    # Use MobileNetV2 as the base model\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(*IMAGE_SIZE, 3), include_top=False)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(*IMAGE_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(x)  # Convolutional layer for processing\n",
    "    \n",
    "    # Add transpose convolution layers to upscale the feature map to the original size\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # Upsample to 16x16\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # Upsample to 32x32\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # Upsample to 64x64\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # Upsample to 128x128\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # Upsample to 256x256\n",
    "    \n",
    "    # Add a final 1x1 convolution to predict the binary mask\n",
    "    x = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(x)  # Output layer for segmentation mask\n",
    "    \n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caffdbd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:54.834446Z",
     "iopub.status.busy": "2025-01-27T17:10:54.834229Z",
     "iopub.status.idle": "2025-01-27T17:10:54.837069Z",
     "shell.execute_reply": "2025-01-27T17:10:54.836491Z"
    },
    "papermill": {
     "duration": 0.007303,
     "end_time": "2025-01-27T17:10:54.838342",
     "exception": false,
     "start_time": "2025-01-27T17:10:54.831039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data generator for training\n",
    "train_gen = data_generator(IMAGE_DIR, MASK_DIR, BATCH_SIZE)\n",
    "# Data generator for testing (used as validation)\n",
    "test_gen = data_generator(TEST_IMAGE_DIR, TEST_MASK_DIR, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4736e516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:54.845048Z",
     "iopub.status.busy": "2025-01-27T17:10:54.844834Z",
     "iopub.status.idle": "2025-01-27T17:10:54.964468Z",
     "shell.execute_reply": "2025-01-27T17:10:54.963724Z"
    },
    "papermill": {
     "duration": 0.124441,
     "end_time": "2025-01-27T17:10:54.965823",
     "exception": false,
     "start_time": "2025-01-27T17:10:54.841382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(804, 124)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate steps per epoch (number of batches in one epoch)\n",
    "train_steps = len(os.listdir(IMAGE_DIR)) // BATCH_SIZE\n",
    "test_steps = len(os.listdir(TEST_IMAGE_DIR)) // BATCH_SIZE\n",
    "\n",
    "train_steps, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0928df30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:10:54.972616Z",
     "iopub.status.busy": "2025-01-27T17:10:54.972392Z",
     "iopub.status.idle": "2025-01-27T17:11:00.921113Z",
     "shell.execute_reply": "2025-01-27T17:11:00.920394Z"
    },
    "papermill": {
     "duration": 5.953435,
     "end_time": "2025-01-27T17:11:00.922423",
     "exception": false,
     "start_time": "2025-01-27T17:10:54.968988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-a670d2c81b25>:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = tf.keras.applications.MobileNetV2(input_shape=(*IMAGE_SIZE, 3), include_top=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,949,376</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,949,376\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose (\u001b[38;5;33mConv2DTranspose\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m295,040\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_1 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_2 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_3 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_transpose_4 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,092,865</span> (23.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,092,865\u001b[0m (23.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,834,881</span> (14.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,834,881\u001b[0m (14.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_deeplabv3plus_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241c52d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:11:00.933020Z",
     "iopub.status.busy": "2025-01-27T17:11:00.932792Z",
     "iopub.status.idle": "2025-01-27T17:11:00.943732Z",
     "shell.execute_reply": "2025-01-27T17:11:00.943123Z"
    },
    "papermill": {
     "duration": 0.017578,
     "end_time": "2025-01-27T17:11:00.945030",
     "exception": false,
     "start_time": "2025-01-27T17:11:00.927452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af43f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:11:00.955562Z",
     "iopub.status.busy": "2025-01-27T17:11:00.955305Z",
     "iopub.status.idle": "2025-01-27T18:14:17.533301Z",
     "shell.execute_reply": "2025-01-27T18:14:17.532414Z"
    },
    "papermill": {
     "duration": 3796.584586,
     "end_time": "2025-01-27T18:14:17.534614",
     "exception": false,
     "start_time": "2025-01-27T17:11:00.950028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1026s\u001b[0m 1s/step - accuracy: 0.8978 - loss: 0.3011 - val_accuracy: 0.9417 - val_loss: 0.2174\n",
      "Epoch 2/5\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 881ms/step - accuracy: 0.9442 - loss: 0.2035 - val_accuracy: 0.9444 - val_loss: 0.1778\n",
      "Epoch 3/5\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 868ms/step - accuracy: 0.9501 - loss: 0.1509 - val_accuracy: 0.9455 - val_loss: 0.1330\n",
      "Epoch 4/5\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 849ms/step - accuracy: 0.9544 - loss: 0.1079 - val_accuracy: 0.9447 - val_loss: 0.1309\n",
      "Epoch 5/5\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 849ms/step - accuracy: 0.9577 - loss: 0.0999 - val_accuracy: 0.9417 - val_loss: 0.1437\n",
      "Training complete. Weights saved in /kaggle/working/checkpoints/\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard callback\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "# Model checkpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(CHECKPOINT_DIR, 'model_weights.weights.h5'),\n",
    "                                                    save_weights_only=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    monitor='loss',\n",
    "                                                    mode='min')\n",
    "    \n",
    "# Training\n",
    "model.fit(train_gen, steps_per_epoch=train_steps, epochs=EPOCHS, \n",
    "            validation_data=test_gen, validation_steps=test_steps,\n",
    "            callbacks=[tensorboard_cb, checkpoint_cb])\n",
    "\n",
    "\n",
    "print(f\"Training complete. Weights saved in {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3885e6cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T18:14:17.912626Z",
     "iopub.status.busy": "2025-01-27T18:14:17.912308Z",
     "iopub.status.idle": "2025-01-27T18:14:18.483353Z",
     "shell.execute_reply": "2025-01-27T18:14:18.482527Z"
    },
    "papermill": {
     "duration": 0.758259,
     "end_time": "2025-01-27T18:14:18.484643",
     "exception": false,
     "start_time": "2025-01-27T18:14:17.726384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "print(cv2.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4196dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T18:14:18.858934Z",
     "iopub.status.busy": "2025-01-27T18:14:18.858617Z",
     "iopub.status.idle": "2025-01-27T18:14:19.171529Z",
     "shell.execute_reply": "2025-01-27T18:14:19.170399Z"
    },
    "papermill": {
     "duration": 0.500646,
     "end_time": "2025-01-27T18:14:19.172722",
     "exception": true,
     "start_time": "2025-01-27T18:14:18.672076",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CROPPED_SAVE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2fe91fe49cfc>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_and_save_cropped_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCROPPED_SAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcropped_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No segmented area to crop and save.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CROPPED_SAVE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_image_for_prediction(img_path):\n",
    "    img = load_img(img_path, target_size=IMAGE_SIZE)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    return img_array[np.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "\n",
    "# def crop_segmented_area(original_image, mask):\n",
    "#     # BOUNDING BOX CUTTING\n",
    "#     # Threshold the mask to binary format\n",
    "#     binary_mask = (mask[0, ..., 0] > 0.5).astype(np.uint8)  # Shape: (256, 256)\n",
    "    \n",
    "#     # Find contours in the binary mask\n",
    "#     contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "#     if len(contours) == 0:\n",
    "#         print(\"No segmented area detected.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Find the bounding box for the largest contour\n",
    "#     largest_contour = max(contours, key=cv2.contourArea)\n",
    "#     x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "#     # Crop the original image to the bounding box\n",
    "#     original_image_resized = cv2.resize(original_image, IMAGE_SIZE)  # Resize to match the model's input size\n",
    "#     cropped_image = original_image_resized[y:y + h, x:x + w]\n",
    "#     return cropped_image\n",
    "\n",
    "# def crop_segmented_area(original_image, mask):\n",
    "#     # EXACT CUTTING\n",
    "#     # Threshold the mask to binary format\n",
    "#     binary_mask = (mask[0, ..., 0] > 0.5).astype(np.uint8)  # Shape: (256, 256)\n",
    "\n",
    "#     # Ensure the mask is the same size as the original image\n",
    "#     binary_mask_resized = cv2.resize(binary_mask, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "#     # Create an empty mask the same size as the input image\n",
    "#     segmentation_mask = np.zeros_like(binary_mask_resized)\n",
    "\n",
    "#     # Fill the mask with the largest segmented area\n",
    "#     contours, _ = cv2.findContours(binary_mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     if len(contours) == 0:\n",
    "#         print(\"No segmented area detected.\")\n",
    "#         return None\n",
    "    \n",
    "#     cv2.drawContours(segmentation_mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "#     # Convert original image to uint8 if not already\n",
    "#     original_image_uint8 = np.uint8(original_image * 255) if original_image.max() <= 1 else np.uint8(original_image)\n",
    "\n",
    "#     # Use the mask to keep only the segmented area of the image\n",
    "#     segmented_image = cv2.bitwise_and(original_image_uint8, original_image_uint8, mask=segmentation_mask)\n",
    "    \n",
    "#     # Resize to match the model's input size (if needed)\n",
    "#     segmented_image_resized = cv2.resize(segmented_image, (IMAGE_SIZE[1], IMAGE_SIZE[0]))\n",
    "\n",
    "#     return segmented_image_resized\n",
    "\n",
    "\n",
    "def crop_segmented_area_and_bounding_box(original_image, mask, image_size=(256, 256)):\n",
    "    # Threshold the mask to binary format (use the mask shape)\n",
    "    binary_mask = (mask[0, ..., 0] > 0.5).astype(np.uint8)  # Shape: (256, 256)\n",
    "    \n",
    "    # Resize the binary mask to match the original image size\n",
    "    binary_mask_resized = cv2.resize(binary_mask, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Find contours in the binary mask to get the segmented areas\n",
    "    contours, _ = cv2.findContours(binary_mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        print(\"No segmented area detected.\")\n",
    "        return None  # Return None if no contours are found\n",
    "    \n",
    "    # Find the bounding box surrounding the largest contour\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "    # Optionally, visualize the bounding box if needed:\n",
    "    cv2.rectangle(original_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Now, crop the original image using the bounding box directly (without resizing)\n",
    "    cropped_image = original_image[y:y+h, x:x+w]\n",
    "\n",
    "    # Optionally, resize the cropped image to match the model's input size (e.g., 256x256)\n",
    "    cropped_image_resized = cv2.resize(cropped_image, image_size)\n",
    "\n",
    "    return cropped_image_resi\n",
    "\n",
    "\n",
    "def visualize_and_save_cropped_result(original_image, cropped_image, save_path=CROPPED_SAVE_PATH):\n",
    "    if cropped_image is None:\n",
    "        print(\"No segmented area to crop and save.\")\n",
    "        return\n",
    "    \n",
    "    # Save the cropped result\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(\"Cropped Segmented Area\")\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.axis(\"off\")\n",
    "    #plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    print(f\"Cropped segmented area saved at {save_path}\")\n",
    "\n",
    "def load_and_predict(image_path):\n",
    "    # Load model architecture\n",
    "    model = build_deeplabv3plus_model()\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_weights(CHECKPOINT_DIR+'model_weights.weights.h5')\n",
    "    print(f\"Weights loaded from {CHECKPOINT_DIR}\")\n",
    "    \n",
    "    # Preprocess the input image\n",
    "    img = preprocess_image_for_prediction(image_path)\n",
    "    \n",
    "    # Predict mask\n",
    "    mask = model.predict(img)\n",
    "    \n",
    "    # Load the original image (not resized)\n",
    "    original_img = img_to_array(load_img(image_path)) / 255.0  # Original scale\n",
    "    \n",
    "    # Crop segmented area\n",
    "    cropped_image = crop_segmented_area(original_img, mask)\n",
    "    visualize_and_save_cropped_result(original_img, cropped_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90a670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:09:07.937834Z",
     "iopub.status.busy": "2025-01-27T17:09:07.937518Z",
     "iopub.status.idle": "2025-01-27T17:09:11.359837Z",
     "shell.execute_reply": "2025-01-27T17:09:11.358671Z",
     "shell.execute_reply.started": "2025-01-27T17:09:07.937808Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = '/kaggle/input/indian-driving-dataset-segmentation-all-level1id/idd/test/images/0000266_leftImg8bit.jpg'\n",
    "load_and_predict(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6b41d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16a346",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5221545,
     "sourceId": 8705227,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3824.376168,
   "end_time": "2025-01-27T18:14:22.580750",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-27T17:10:38.204582",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
